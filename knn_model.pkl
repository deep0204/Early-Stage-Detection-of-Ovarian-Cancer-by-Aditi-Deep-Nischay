xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=1)

# Step 1: Basic KNN Model (with n_neighbors=5)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(xtrain, ytrain)
ypred = knn.predict(xtest)

# Evaluate the basic KNN model
print("Basic KNN Model Evaluation:")
print(confusion_matrix(ytest, ypred))
print(classification_report(ytest, ypred))
error_rate = []
k_values = range(1, 40)

for i in k_values:
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(xtrain, ytrain)
    pred_i = knn.predict(xtest)
    error_rate.append(np.mean(pred_i != ytest))

# Find the k value with the lowest error rate
optimal_k = k_values[np.argmin(error_rate)]
min_error_rate = np.min(error_rate)

# Print the optimal k and corresponding error rate
print(f"The optimal k value is {optimal_k} with the lowest error rate of {min_error_rate:.4f}")

# Plotting K vs Error rate
plt.figure(figsize=(10,6))
plt.plot(k_values, error_rate, color='blue', linestyle='--', markersize=10, markerfacecolor='red', marker='o')
plt.title('K vs Error Rate')
plt.xlabel('K')
plt.ylabel('Error Rate')
plt.show()
# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# KNN hyperparameter tuning
param_dist_knn = {'n_neighbors': range(1, 11)}
knn = RandomizedSearchCV(KNeighborsClassifier(), param_dist_knn, n_iter=5, cv=3, n_jobs=-1, random_state=42)
knn.fit(X_train, y_train)
best_knn = knn.best_estimator_

# Print the best k value (this is the value of n_neighbors that gives the best performance)
print(f"The best k value is: {knn.best_params_['n_neighbors']}")

# Evaluation function
def evaluate_model(model, X_train, y_train, X_test, y_test):
    y_pred = model.predict(X_test)
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted'),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1 Score': f1_score(y_test, y_pred, average='weighted'),
        'AUC': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]),
        'Log Loss': log_loss(y_test, model.predict_proba(X_test)),
    }
    print(confusion_matrix(y_test, y_pred))
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

# Evaluate best KNN model
print("\nEvaluating KNN Model...")
evaluate_model(best_knn, X_train, y_train, X_test, y_test)

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

# Assuming best_knn is already fitted with the best k value from RandomizedSearchCV

# Perform permutation importance
perm_importance = permutation_importance(best_knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

# Extract feature importance values
importance = perm_importance.importances_mean

# Sort features by importance
sorted_idx = importance.argsort()

# Select the top 3 features
top_3_idx = sorted_idx[-3:]

# Plotting the top 3 feature importances
plt.figure(figsize=(10, 6))
plt.barh(X.columns[top_3_idx], importance[top_3_idx], align='center', color='skyblue')
plt.xlabel("Permutation Importance")
plt.title("Top 3 Feature Importance (Permutation) for KNN Model")
plt.show()
# Step 1: Apply PCA to reduce dimensions and extract the most important features
pca = PCA(n_components=2)  # Reduce to 2 components for visualization
X_pca = pca.fit_transform(X)

# Step 2: Visualize the Explained Variance Ratio of PCA Components
plt.figure(figsize=(8, 6))
plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, alpha=0.7, align='center')
plt.xlabel('Principal Components')
plt.ylabel('Variance Ratio')
plt.title('Explained Variance of PCA Components')
plt.show()

# Step 3: Visualize the Transformed Data in 2D (PCA)
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.title('PCA of Dataset (Transformed Data)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar()  # Add color bar for target classes
plt.show()

# Step 4: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Step 5: Train the KNN Model on the PCA-transformed data
knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(X_train, y_train)

# Step 6: Make predictions and evaluate the model
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# Step 7: Print the evaluation metrics for the model
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Train Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Confusion Matrix
print("Training Confusion Matrix:")
print(confusion_matrix(y_train, y_train_pred))

print("Testing Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# Optional: You can visualize the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for KNN Model (Test Data)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
