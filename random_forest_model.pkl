import re
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc

from sklearn.metrics import accuracy_score, confusion_matrix , precision_score, recall_score, f1_score, roc_auc_score, log_loss  # Import metrics

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, log_loss, confusion_matrix
)

# Assuming 'combined_data' is already defined and has 'TYPE' and 'TYPE.1' columns
X = combined_data.drop(['TYPE', 'TYPE.1'], axis=1)  # Features
y = combined_data['TYPE']  # Target

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Classifier with GridSearch for Best Model
rfc = RandomForestClassifier(random_state=42)
param_grid_rfc = {
    'n_estimators': [200, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [4, 5, 6, 7, 8],
    'criterion': ['gini', 'entropy']
}
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid_rfc, cv=5)
CV_rfc.fit(X_train, y_train)

# Best RFC model
m_best_rfc = CV_rfc.best_estimator_
y_pred_rfc = m_best_rfc.predict(X_test)
y_train_pred_rfc = m_best_rfc.predict(X_train)

# Metrics for Best RFC Model
train_accuracy = accuracy_score(y_train, y_train_pred_rfc)
test_accuracy = accuracy_score(y_test, y_pred_rfc)
precision = precision_score(y_test, y_pred_rfc, average='weighted', zero_division=1)
recall = recall_score(y_test, y_pred_rfc, average='weighted')
f1 = f1_score(y_test, y_pred_rfc, average='weighted')
auc = roc_auc_score(y_test, m_best_rfc.predict_proba(X_test)[:, 1])
logloss = log_loss(y_test, m_best_rfc.predict_proba(X_test))

# Printing Evaluation Results
print(f"Best RFC Model Evaluation:")
print("Training confusion matrix:")
print(confusion_matrix(y_train, y_train_pred_rfc))
print("Testing confusion matrix:")
print(confusion_matrix(y_test, y_pred_rfc))
print(f"Train Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"AUC: {auc:.4f}")
print(f"Log Loss: {logloss:.4f}")
# Random Forest Classifier (Overfitting example)
rfc_overfit = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=42)
rfc_overfit.fit(X_train, y_train)
y_pred_rfc_overfit = rfc_overfit.predict(X_test)
rfc_overfit_train_acc = accuracy_score(y_train, rfc_overfit.predict(X_train))
rfc_overfit_test_acc = accuracy_score(y_test, y_pred_rfc_overfit)

# Print the results for the overfitting scenario
print(f"Random Forest (Overfitting):")
print(f"Train Accuracy: {rfc_overfit_train_acc:.4f}")
print(f"Test Accuracy: {rfc_overfit_test_acc:.4f}")
print(confusion_matrix(y_train, rfc_overfit.predict(X_train)))
print(confusion_matrix(y_test, y_pred_rfc_overfit))


# Random Forest Classifier (Underfitting example)
rfc_underfit = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42)
rfc_underfit.fit(X_train, y_train)
y_pred_rfc_underfit = rfc_underfit.predict(X_test)
rfc_underfit_train_acc = accuracy_score(y_train, rfc_underfit.predict(X_train))
rfc_underfit_test_acc = accuracy_score(y_test, y_pred_rfc_underfit)

# Print the results for the underfitting scenario
print(f"Random Forest (Underfitting):")
print(f"Train Accuracy: {rfc_underfit_train_acc:.4f}")
print(f"Test Accuracy: {rfc_underfit_test_acc:.4f}")
print(confusion_matrix(y_train, rfc_underfit.predict(X_train)))
print(confusion_matrix(y_test, y_pred_rfc_underfit))

# Data for comparison
models = ['Best RFC', 'Random Forest (Overfitting)', 'Random Forest (Underfitting)']
train_accuracies = [m_best_rfc_train_acc, rfc_overfit_train_acc, rfc_underfit_train_acc]
test_accuracies = [m_best_rfc_test_acc, rfc_overfit_test_acc, rfc_underfit_test_acc]

# Plotting
plt.figure(figsize=(10, 6))
x = np.arange(len(models))  # X-axis positions
width = 0.35  # Bar width

# Plot bars for Train and Test Accuracies
bars_train = plt.bar(x - width/2, train_accuracies, width, label='Train Accuracy')
bars_test = plt.bar(x + width/2, test_accuracies, width, label='Test Accuracy')

# Adding labels and title
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Comparison of Train and Test Accuracies for RFC Models')
plt.xticks(x, models)
plt.legend()

# Adding values on top of the bars
for bar in bars_train:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,  # Positioning text slightly above the bar
             f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=10)

for bar in bars_test:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,  # Positioning text slightly above the bar
             f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=10)

# Display the plot
plt.tight_layout()
plt.show()

# Assuming `m_best_rfc` is your trained Random Forest model
importances = m_best_rfc.feature_importances_

# Create a DataFrame to display feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the top three most important features
top_features = feature_importance_df.head(3)
print("Top Three Important Features for RFC Model:")
print(top_features)

# Plotting Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=top_features)  # Removed palette
plt.title('Top Three Most Important Features for RFC Model')
plt.show()
# Step 1: Apply PCA to reduce dimensions and extract the most important features
pca = PCA(n_components=2)  # Reduce to 2 components for visualization and model input
X_pca = pca.fit_transform(X)

# Step 2: Visualize the Explained Variance Ratio of PCA Components
plt.figure(figsize=(8, 6))
plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, alpha=0.7, align='center')
plt.xlabel('Principal Components')
plt.ylabel('Variance Ratio')
plt.title('Explained Variance of PCA Components')
plt.show()

# Step 3: Visualize the Transformed Data in 2D
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.title('PCA of Dataset (Transformed Data)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar()  # Add color bar for target classes
plt.show()

# Step 4: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Step 5: Train the Random Forest Classifier on the PCA-transformed data
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train, y_train)

# Step 6: Make predictions and evaluate the model
y_train_pred = rfc.predict(X_train)
y_test_pred = rfc.predict(X_test)

# Step 7: Print the evaluation metrics for the model
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Train Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Confusion Matrix
print("Training Confusion Matrix:")
print(confusion_matrix(y_train, y_train_pred))

print("Testing Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# Optional: You can visualize the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for Test Data')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()